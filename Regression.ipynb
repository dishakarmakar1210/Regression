{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "--> Simple Linear Regression is a statistical method used to model the relationship between two variables: one independent variable (X) and one dependent variable (Y). It fits a straight line (Y = mX + c) through the data to predict Y based on X. The goal is to find the best-fitting line that minimizes prediction errors.\n",
        "\n",
        "---\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "--> The main assumptions are:\n",
        "\n",
        "(1) Linearity – the relationship between X and Y is linear,\n",
        "\n",
        "(2) Independence – observations are independent of each other,\n",
        "\n",
        "(3) Homoscedasticity – constant variance of errors, and\n",
        "\n",
        "(4) Normality – residuals (errors) are normally distributed. Violating these can affect model performance and reliability.\n",
        "\n",
        "---\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "--> The coefficient m is the slope of the regression line. It indicates the change in the dependent variable Y for every one-unit increase in the independent variable X. A positive m means a positive relationship; negative m indicates a negative relationship.\n",
        "\n",
        "---\n",
        "\n",
        "4. What does the intercept c represent in the equation Y = mX + c?\n",
        "--> The intercept c is the value of Y when X is zero. It represents the point where the regression line crosses the Y-axis. It gives a baseline value for Y before any effect of X is applied.\n",
        "\n",
        "---\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "--> The slope m is calculated using the formula:\n",
        "\n",
        "m = Σ[(X - mean of X)(Y - mean of Y)] / Σ[(X - mean of X)²].\n",
        "\n",
        "\n",
        "It measures the direction and steepness of the line by estimating how changes in X relate to changes in Y.\n",
        "\n",
        "---\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "--> The least squares method finds the best-fitting line by minimizing the sum of the squared differences between observed values and predicted values. This helps ensure that the overall prediction error is as small as possible, leading to a more accurate model.\n",
        "\n",
        "---\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "--> R² measures how well the regression line explains the variability in the dependent variable. It ranges from 0 to 1, where a higher value indicates a better fit. For example, an R² of 0.8 means 80% of the variance in Y is explained by X.\n",
        "\n",
        "---\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "--> Multiple Linear Regression is an extension of simple linear regression that models the relationship between one dependent variable and two or more independent variables. The equation is Y = b₀ + b₁X₁ + b₂X₂ + ... + bₙXₙ. It helps in understanding how multiple factors simultaneously influence the outcome.\n",
        "\n",
        "---\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "--> The key difference is the number of independent variables. Simple linear regression uses only one independent variable, while multiple linear regression uses two or more. This allows multiple regression to model more complex relationships.\n",
        "\n",
        "---\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "--> Key assumptions include:\n",
        "\n",
        "(1) Linearity between dependent and independent variables,\n",
        "\n",
        "(2) Independence of observations,\n",
        "\n",
        "(3) Homoscedasticity (constant variance of residuals),\n",
        "\n",
        "(4) Normal distribution of residuals, and\n",
        "\n",
        "(5) No multicollinearity – independent variables should not be too highly correlated with each other.\n",
        "\n",
        "---\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "--> Heteroscedasticity occurs when the variance of the residuals (errors) is not constant across all levels of the independent variables. This violates a key regression assumption and can lead to inefficient estimates and biased standard errors. As a result, hypothesis tests (like t-tests) may become unreliable, affecting confidence intervals and p-values.\n",
        "\n",
        "---\n",
        "12.  How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "--> To reduce multicollinearity, you can remove or combine highly correlated predictors, use dimensionality reduction techniques like PCA, or apply regularization methods such as Ridge or Lasso regression. Variance Inflation Factor (VIF) can help identify problematic variables. Addressing multicollinearity improves model stability and interpretation.\n",
        "\n",
        "---\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "--> Common techniques include one-hot encoding (creating binary variables for each category) and label encoding (assigning numeric codes to categories). One-hot encoding is preferred for nominal (unordered) data, while label encoding may be used for ordinal data. These transformations allow categorical data to be used in numerical regression models.\n",
        "\n",
        "---\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "--> Interaction terms capture the combined effect of two or more independent variables on the dependent variable. They help identify whether the impact of one predictor changes depending on the level of another. Including interaction terms can reveal deeper insights and improve model accuracy if such relationships exist.\n",
        "\n",
        "---\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "--> In Simple Linear Regression, the intercept represents the predicted value of Y when X is 0. In Multiple Linear Regression, it represents the expected value of Y when all independent variables are zero, which may not always be meaningful if those values are outside the data range. Its interpretation depends on the context and variable scales.\n",
        "\n",
        "---\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "--> The slope represents the expected change in the dependent variable for a one-unit increase in the independent variable, holding other variables constant (in multiple regression). It quantifies the strength and direction of the relationship. Accurate slopes ensure valid predictions and help determine the influence of predictors.\n",
        "\n",
        "---\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "--> he intercept sets the baseline value of the dependent variable when all predictors are zero. While it may not always have a practical interpretation, it helps anchor the regression line and provides a starting point for understanding how other variables affect the outcome. Its value can also affect the model’s overall prediction.\n",
        "\n",
        "---\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "--> R² only tells how much variance in the dependent variable is explained by the model; it doesn’t indicate whether the model is accurate or appropriate. A high R² doesn’t guarantee good predictive performance or that the model captures the true relationship. It also doesn’t account for overfitting or model complexity—adjusted R² or cross-validation may provide better insights.\n",
        "\n",
        "---\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "--> A large standard error indicates that the estimate of the coefficient is imprecise. It suggests that the coefficient may not be significantly different from zero, which can lead to wide confidence intervals and higher p-values. This may result from multicollinearity, small sample size, or high data variability.\n",
        "\n",
        "---\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "--> Heteroscedasticity can be spotted in residual plots when the spread of residuals increases or decreases with fitted values, often forming a fan or cone shape. It violates regression assumptions, leading to inefficient estimators and unreliable statistical tests. Addressing it (e.g., via transformation or robust regression) ensures more valid inference and better model performance.\n",
        "\n",
        "---\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "--> It means that while the model appears to explain a large portion of variance in the target variable (high R²), many of the predictors may not be actually useful. Adjusted R² penalizes the addition of irrelevant variables, so a low adjusted R² suggests overfitting due to unnecessary or non-significant features. It’s a sign the model may not generalize well to new data.\n",
        "\n",
        "---\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "--> Scaling is important because variables on different scales can bias the model, especially when regularization techniques like Ridge or Lasso are used. It helps the optimization process converge faster and allows coefficients to be more meaningfully compared. Scaling also improves numerical stability in calculations.\n",
        "\n",
        "---\n",
        "23. What is polynomial regression?\n",
        "--> Polynomial regression is a type of regression that models the relationship between the independent variable(s) and the dependent variable as an nth-degree polynomial. It can capture non-linear patterns by including powers of the independent variable (e.g., X², X³). It's still considered a form of linear regression because the model is linear in the coefficients.\n",
        "\n",
        "---\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "--> Linear regression models a straight-line relationship between variables, while polynomial regression models curved relationships by adding higher-degree terms (e.g., X², X³). This allows polynomial regression to fit more complex, non-linear trends. However, it increases the risk of overfitting if not properly controlled.\n",
        "\n",
        "---\n",
        "25. When is polynomial regression used?\n",
        "--> Polynomial regression is used when the data shows a non-linear trend that a straight line can't fit well. It’s particularly useful for capturing curves, bends, or fluctuations in relationships between variables. It's often applied when visualizations suggest a parabolic or more complex pattern.\n",
        "\n",
        "---\n",
        "26. What is the general equation for polynomial regression?\n",
        "--> The general equation for polynomial regression is:\n",
        "Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βₙXⁿ + ε\n",
        "Here, n is the degree of the polynomial, and β coefficients are the parameters to be learned.\n",
        "\n",
        "---\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "--> Yes, polynomial regression can be extended to multiple variables by including interaction and power terms of each predictor. For example, it may include terms like X₁², X₂X₃, or X₁³. This increases the model’s flexibility but also the complexity and risk of overfitting.\n",
        "\n",
        "---\n",
        "28. What are the limitations of polynomial regression?\n",
        "--> Polynomial regression is prone to overfitting, especially with high-degree polynomials. It can also become computationally expensive and sensitive to outliers. Additionally, extrapolation beyond the range of the data can produce unrealistic predictions due to the curve's rapid changes.\n",
        "\n",
        "---\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "--> You can use cross-validation, adjusted R², AIC/BIC, and residual plots to evaluate model fit. Cross-validation helps assess generalization, while adjusted R² penalizes unnecessary complexity. Residual analysis helps ensure that adding degrees actually improves model performance.\n",
        "\n",
        "---\n",
        "30.  Why is visualization important in polynomial regression?\n",
        "--> Visualization helps you understand whether the polynomial model fits the data appropriately. It reveals overfitting, underfitting, or unnatural curves introduced by high-degree terms. Plotting predictions against actual data makes it easier to judge model behavior and accuracy.\n",
        "\n",
        "---\n",
        "31. How is polynomial regression implemented in Python?\n",
        "--> In Python, polynomial regression is commonly implemented using PolynomialFeatures from scikit-learn, followed by fitting a LinearRegression model.\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression().fit(X_poly, y)\n"
      ],
      "metadata": {
        "id": "PgKr3F9pwFBn"
      }
    }
  ]
}